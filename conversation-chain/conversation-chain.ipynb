{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of langchain chat memory buffers\n",
    "- ConversationBufferMemory, save previous conversations to chat history\n",
    "- ConversationSummaryMemory, summarize the previous conversations\n",
    "- ConversationBufferWindowMemory, fix window size\n",
    "- ConversationKGMemory, knowledge graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a chatbot using langchain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY, \n",
    "    temperature=0,\n",
    "    model=\"gpt-4\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual prompt with LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
    "\n",
    "{history}\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=['history', 'human_input'], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum physics studies particles at the smallest scales. -Dr.roboto\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "output = chain.run(human_input=\"what is quantum physics? Answer in under 50 characters. sign off your message with -Dr.roboto\", history=\"\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationChain with predefined prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY, \n",
    "    temperature=0,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "\n",
    "conversation = ConversationChain(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is quantum physics? Answer in under 50 characters. sign off your message with -Dr.roboto',\n",
       " 'history': '',\n",
       " 'response': 'The study of subatomic particles and their interactions. -Dr.roboto'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"what is quantum physics? Answer in under 50 characters. sign off your message with -Dr.roboto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_buffer = ConversationChain(llm=llm, memory=ConversationBufferMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_buffer.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "def chat_and_count_tokens(chain, query: str) -> int:\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f\"Spent a total of {cb.total_tokens} tokens.\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 376 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's a great topic! Integrating large language models with external knowledge can have numerous benefits. By combining the power of language models with external knowledge sources, you can enhance the model's understanding and generate more accurate and contextually relevant responses.\\n\\nThere are several approaches you can take to integrate external knowledge with large language models. One common method is to use knowledge graphs, which represent information in a structured format. By connecting the language model to a knowledge graph, you can enable it to access and reason over a vast amount of factual information.\\n\\nAnother approach is to leverage pre-trained models that have been specifically trained on domain-specific knowledge. For example, you can fine-tune a language model on a dataset that contains information from a specific domain, such as medicine or law. This allows the model to have a deeper understanding of the domain-specific concepts and terminology.\\n\\nAdditionally, you can use techniques like information retrieval to fetch relevant information from external knowledge sources in real-time. This can be done by querying databases, APIs, or even web scraping. By combining the retrieved information with the language model's understanding, you can generate more informed and accurate responses.\\n\\nIt's worth noting that integrating external knowledge with large language models is an active area of research, and there are ongoing efforts to improve the methods and techniques used. Researchers are constantly exploring ways to make language models more knowledgeable and capable of reasoning over external information.\\n\\nIs there anything specific you would like to know or discuss about integrating large language models with external knowledge?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_and_count_tokens(conversation_with_buffer, \"I want to explore the potential of integrating large language models with external knowledge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 657 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The choice of which approach is better for integrating external knowledge with large language models depends on the specific use case and requirements. Each approach has its own advantages and considerations.\\n\\nUsing knowledge graphs can provide a structured representation of information, allowing for efficient querying and reasoning. This approach is particularly useful when dealing with factual information that can be organized into a graph-like structure. Knowledge graphs can also capture relationships between entities, enabling the language model to make more nuanced and contextually relevant responses.\\n\\nOn the other hand, fine-tuning language models on domain-specific datasets can be beneficial when you need the model to have a deeper understanding of a specific domain. This approach allows the model to learn domain-specific concepts, terminology, and patterns, which can improve its performance in that particular domain. However, fine-tuning requires access to domain-specific data and may not be as effective for broader knowledge integration.\\n\\nInformation retrieval techniques can be useful when you need real-time access to external knowledge sources. By fetching relevant information on the fly, you can provide up-to-date and contextually relevant responses. However, this approach may require additional infrastructure and may not be as efficient as having the knowledge directly integrated into the model.\\n\\nUltimately, the best approach depends on the specific requirements of your application and the available resources. It may be beneficial to experiment with different approaches and evaluate their performance in your specific use case.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_and_count_tokens(conversation_with_buffer, \"Which way is better?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 902 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A knowledge graph is a structured representation of information that captures relationships between entities. It is typically represented as a graph, where nodes represent entities and edges represent relationships between those entities. Each node and edge in the graph can have attributes that provide additional information.\\n\\nKnowledge graphs are designed to organize and represent factual information in a way that is easily queryable and computationally efficient. They can be used to store and reason over a wide range of knowledge, such as facts, concepts, and relationships between entities.\\n\\nFor example, a knowledge graph could represent information about movies, actors, and directors. Each movie, actor, and director would be represented as a node in the graph, and the relationships between them (e.g., \"directed by,\" \"acted in\") would be represented as edges. The nodes and edges can also have attributes, such as the release year of a movie or the birth date of an actor.\\n\\nKnowledge graphs can be built manually by experts or automatically extracted from various sources, such as structured databases, text documents, or the web. They can be used to enhance the understanding and reasoning capabilities of large language models by providing them with a structured representation of external knowledge.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_and_count_tokens(conversation_with_buffer, \"What is a knowledge graph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1295 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure! Here\\'s an example of a simple knowledge graph representing information about movies, actors, and directors:\\n\\nNodes:\\n- Movie: \"The Shawshank Redemption\", \"Inception\", \"The Dark Knight\"\\n- Actor: \"Morgan Freeman\", \"Leonardo DiCaprio\", \"Christian Bale\"\\n- Director: \"Frank Darabont\", \"Christopher Nolan\"\\n\\nEdges:\\n- \"directed by\": connects the Director nodes to the Movie nodes\\n- \"acted in\": connects the Actor nodes to the Movie nodes\\n\\nAttributes:\\n- Movie nodes can have attributes like \"release year\" and \"genre\"\\n- Actor nodes can have attributes like \"birth date\" and \"nationality\"\\n- Director nodes can have attributes like \"birth date\" and \"nationality\"\\n\\nHere\\'s how the knowledge graph would look like:\\n\\n```\\n(Movie) The Shawshank Redemption --[directed by]--> (Director) Frank Darabont\\n(Movie) The Shawshank Redemption --[acted in]--> (Actor) Morgan Freeman\\n\\n(Movie) Inception --[directed by]--> (Director) Christopher Nolan\\n(Movie) Inception --[acted in]--> (Actor) Leonardo DiCaprio\\n\\n(Movie) The Dark Knight --[directed by]--> (Director) Christopher Nolan\\n(Movie) The Dark Knight --[acted in]--> (Actor) Christian Bale\\n```\\n\\nIn this example, the knowledge graph represents the relationships between movies, actors, and directors. The edges indicate the connections between the nodes, such as which director directed which movie and which actor acted in which movie. The attributes associated with each node provide additional information, such as the release year of a movie or the birth date of an actor.\\n\\nThis is a simplified example, but knowledge graphs can be much larger and more complex, capturing a wide range of information and relationships.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_and_count_tokens(conversation_with_buffer, \"Can you show me an example of knowledge graph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: I want to explore the potential of integrating large language models with external knowledge.\n",
      "AI: That's a great topic! Integrating large language models with external knowledge can have numerous benefits. By combining the power of language models with external knowledge sources, you can enhance the model's understanding and generate more accurate and contextually relevant responses.\n",
      "\n",
      "There are several approaches you can take to integrate external knowledge with large language models. One common method is to use knowledge graphs, which represent information in a structured format. By connecting the language model to a knowledge graph, you can enable it to access and reason over a vast amount of factual information.\n",
      "\n",
      "Another approach is to leverage pre-trained models that have been specifically trained on domain-specific knowledge. For example, you can fine-tune a language model on a dataset that contains information from a specific domain, such as medicine or law. This allows the model to have a deeper understanding of the domain-specific concepts and terminology.\n",
      "\n",
      "Additionally, you can use techniques like information retrieval to fetch relevant information from external knowledge sources in real-time. This can be done by querying databases, APIs, or even web scraping. By combining the retrieved information with the language model's understanding, you can generate more informed and accurate responses.\n",
      "\n",
      "It's worth noting that integrating external knowledge with large language models is an active area of research, and there are ongoing efforts to improve the methods and techniques used. Researchers are constantly exploring ways to make language models more knowledgeable and capable of reasoning over external information.\n",
      "\n",
      "Is there anything specific you would like to know or discuss about integrating large language models with external knowledge?\n",
      "Human: Which way is better?\n",
      "AI: The choice of which approach is better for integrating external knowledge with large language models depends on the specific use case and requirements. Each approach has its own advantages and considerations.\n",
      "\n",
      "Using knowledge graphs can provide a structured representation of information, allowing for efficient querying and reasoning. This approach is particularly useful when dealing with factual information that can be organized into a graph-like structure. Knowledge graphs can also capture relationships between entities, enabling the language model to make more nuanced and contextually relevant responses.\n",
      "\n",
      "On the other hand, fine-tuning language models on domain-specific datasets can be beneficial when you need the model to have a deeper understanding of a specific domain. This approach allows the model to learn domain-specific concepts, terminology, and patterns, which can improve its performance in that particular domain. However, fine-tuning requires access to domain-specific data and may not be as effective for broader knowledge integration.\n",
      "\n",
      "Information retrieval techniques can be useful when you need real-time access to external knowledge sources. By fetching relevant information on the fly, you can provide up-to-date and contextually relevant responses. However, this approach may require additional infrastructure and may not be as efficient as having the knowledge directly integrated into the model.\n",
      "\n",
      "Ultimately, the best approach depends on the specific requirements of your application and the available resources. It may be beneficial to experiment with different approaches and evaluate their performance in your specific use case.\n",
      "Human: What is a knowledge graph?\n",
      "AI: A knowledge graph is a structured representation of information that captures relationships between entities. It is typically represented as a graph, where nodes represent entities and edges represent relationships between those entities. Each node and edge in the graph can have attributes that provide additional information.\n",
      "\n",
      "Knowledge graphs are designed to organize and represent factual information in a way that is easily queryable and computationally efficient. They can be used to store and reason over a wide range of knowledge, such as facts, concepts, and relationships between entities.\n",
      "\n",
      "For example, a knowledge graph could represent information about movies, actors, and directors. Each movie, actor, and director would be represented as a node in the graph, and the relationships between them (e.g., \"directed by,\" \"acted in\") would be represented as edges. The nodes and edges can also have attributes, such as the release year of a movie or the birth date of an actor.\n",
      "\n",
      "Knowledge graphs can be built manually by experts or automatically extracted from various sources, such as structured databases, text documents, or the web. They can be used to enhance the understanding and reasoning capabilities of large language models by providing them with a structured representation of external knowledge.\n",
      "Human: Can you show me an example of knowledge graph?\n",
      "AI: Sure! Here's an example of a simple knowledge graph representing information about movies, actors, and directors:\n",
      "\n",
      "Nodes:\n",
      "- Movie: \"The Shawshank Redemption\", \"Inception\", \"The Dark Knight\"\n",
      "- Actor: \"Morgan Freeman\", \"Leonardo DiCaprio\", \"Christian Bale\"\n",
      "- Director: \"Frank Darabont\", \"Christopher Nolan\"\n",
      "\n",
      "Edges:\n",
      "- \"directed by\": connects the Director nodes to the Movie nodes\n",
      "- \"acted in\": connects the Actor nodes to the Movie nodes\n",
      "\n",
      "Attributes:\n",
      "- Movie nodes can have attributes like \"release year\" and \"genre\"\n",
      "- Actor nodes can have attributes like \"birth date\" and \"nationality\"\n",
      "- Director nodes can have attributes like \"birth date\" and \"nationality\"\n",
      "\n",
      "Here's how the knowledge graph would look like:\n",
      "\n",
      "```\n",
      "(Movie) The Shawshank Redemption --[directed by]--> (Director) Frank Darabont\n",
      "(Movie) The Shawshank Redemption --[acted in]--> (Actor) Morgan Freeman\n",
      "\n",
      "(Movie) Inception --[directed by]--> (Director) Christopher Nolan\n",
      "(Movie) Inception --[acted in]--> (Actor) Leonardo DiCaprio\n",
      "\n",
      "(Movie) The Dark Knight --[directed by]--> (Director) Christopher Nolan\n",
      "(Movie) The Dark Knight --[acted in]--> (Actor) Christian Bale\n",
      "```\n",
      "\n",
      "In this example, the knowledge graph represents the relationships between movies, actors, and directors. The edges indicate the connections between the nodes, such as which director directed which movie and which actor acted in which movie. The attributes associated with each node provide additional information, such as the release year of a movie or the birth date of an actor.\n",
      "\n",
      "This is a simplified example, but knowledge graphs can be much larger and more complex, capturing a wide range of information and relationships.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_with_buffer.memory.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationKGMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -qU networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationKGMemory\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=['history', 'input'], template=template)\n",
    "\n",
    "conversation_kg_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=ConversationKGMemory(llm=llm),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: Hi, What's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 1043 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI and I'm here to chat with you. How can I assist you today?\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_and_count_tokens(conversation_kg_chain, \"Hi, What's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: My name is KElvin and I'm helping Will. He's an engineer.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 1166 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Kelvin! It's nice to meet you. How can I assist you with helping Will?\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_and_count_tokens(conversation_kg_chain, \"My name is Kelvin and I'm helping Will. He's an engineer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "On Will: Will is an engineer.\n",
      "\n",
      "Conversation:\n",
      "Human: What do you know about will?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 1140 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Will is an engineer.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_and_count_tokens(conversation_kg_chain, \"What do you know about will?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kelvin', 'Will', 'is helping'), ('Will', 'engineer', 'is an')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_kg_chain.memory.kg.get_triples()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "conversation_sum_chain = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryMemory(llm=llm),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, What's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 323 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI designed to assist with various tasks and provide information. Right now, I'm here to chat with you. How can I help you today?\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_and_count_tokens(conversation_sum_chain, \"Hi, What's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI and asks what it is doing. The AI explains that it is an AI designed to assist with tasks and provide information, and it is currently available to chat and help the human.\n",
      "Human: My name is Kelvin and I'm helping Will. He's an engineer.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 433 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Kelvin! It's nice to meet you. How can I assist you with helping Will, the engineer?\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_and_count_tokens(conversation_sum_chain, \"My name is Kelvin and I'm helping Will. He's an engineer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI and asks what it is doing. The AI explains that it is an AI designed to assist with tasks and provide information, and it is currently available to chat and help the human. The human introduces themselves as Kelvin and mentions that they are helping Will, an engineer. The AI greets Kelvin and asks how it can assist with helping Will.\n",
      "Human: What do you know about will?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 598 tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Will is an engineer who you mentioned you are helping. As an AI, I don't have personal knowledge about individuals unless it has been shared with me in our conversation. However, I can assist you with general information about engineering or any specific questions you may have about Will's work or field of expertise.\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_and_count_tokens(conversation_sum_chain, \"What do you know about will?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

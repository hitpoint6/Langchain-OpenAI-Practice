{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hitpoint6/ai-llm-engineering-projects/blob/main/LLaMa_2_70B_Chatbot_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reg90xDzmLWm"
      },
      "source": [
        "# LLaMa-2 70B Chatbot Agent\n",
        "\n",
        "One of the most important bechmarks for a LLM is the ability to use as Agent and Tools. The open source models so far has not been able to meet the goal. When Llama2 is released. It is claimed to outperform many opensource models. This notebook will test it as an Agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQybjAN1l5SV"
      },
      "source": [
        "Note that running this on CPU is practically impossible. It will take a very long time. If running on Google Colab you go to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > A100**. Using this notebook requires ~38GB of GPU RAM.\n",
        "\n",
        "The model is located at [HuggingFace](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). You may need:\n",
        "\n",
        "1. Request to access to Llama 2 model\n",
        "2. Hugging face access token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LxOELFalhEK",
        "outputId": "a5b6d623-8d1c-4518-cab6-0979e3a77180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement getpass (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for getpass\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    transformers==4.31.0 \\\n",
        "    accelerate==0.21.0 \\\n",
        "    einops==0.6.1 \\\n",
        "    langchain==0.0.240 \\\n",
        "    xformers==0.0.20 \\\n",
        "    bitsandbytes==0.41.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4z6sLR9mJVO"
      },
      "source": [
        "### Initializing the Hugging Face Pipeline\n",
        "\n",
        "Load a 70B parameter model, using the standard weight datatype (float32, 32bits), the total memory needed is 280 GB. As A100 only has 40GB, we will need to convert it to int(4bits) datatype with bit and bytes quantization.\n",
        "\n",
        "But a lot of precision will be lost. We will divide it into parts using int4 and parts using bfloat16 as a compromise.\n",
        "\n",
        "The model took me 15 minutes to load. Consumed 37.4GB GPU RAM and 155.6 GB disks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0qOj8bJmF3r"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "hf_auth = '<Your Hugging Face Auth Token>'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model.eval()  # The model is used for inference not training\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kwBHwvDCnZqj"
      },
      "outputs": [],
      "source": [
        "# For translating text to tokens\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veP7wTcbr5Jc"
      },
      "source": [
        "Start to initialize the HuggingFace pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "325aS9_Vrmmi"
      },
      "outputs": [],
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsZXSZFUsBsy"
      },
      "source": [
        "Confirm the model is working\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq-2d2XCr289",
        "outputId": "eafb8a04-ba47-43a9-d0c2-74e04e0e6a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why summer is hotter than winter?\n",
            "\n",
            "Answer: Summer is hotter than winter because the Earth's axis is tilted, and during the summer months in the Northern Hemisphere, the sun's rays hit the Earth more directly. This increased exposure to direct sunlight leads to higher temperatures. Additionally, the longer days in the Northern Hemisphere during summer also contribute to the warmer temperatures. In contrast, during the winter months, the sun's rays hit the Earth at a more oblique angle, resulting in colder temperatures.\n"
          ]
        }
      ],
      "source": [
        "res = generate_text(\"Why summer is hotter than winter?\")\n",
        "print(res[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vIhszxnsD0h",
        "outputId": "ad650aac-8ee1-4f6f-82c9-f28876e4d491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write a python code to generate the nth fibonacci number?\n",
            "\n",
            "Answer: Sure! Here's an example of how you could write a Python function to generate the nth Fibonacci number:\n",
            "```\n",
            "def fibonacci(n):\n",
            "    if n <= 1:\n",
            "        return n\n",
            "    else:\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "```\n",
            "This function uses a recursive approach, where it calls itself with the arguments `n-1` and `n-2` to calculate the previous two Fibonacci numbers. The base case is when `n` is 1 or less, in which case the function returns `n` directly.\n",
            "\n",
            "Here's an example of how you could use this function to generate the 5th Fibonacci number:\n",
            "```\n",
            "print(fibonacci(5))  # Output: 5\n",
            "```\n",
            "And here's an example of how you could use this function to generate the 10th Fibonacci number:\n",
            "```\n",
            "print(fibonacci(10))  # Output: 55\n",
            "```\n",
            "Note that this function has a time complexity of O(2^n), which means that the time it takes to calculate the nth Fibonacci number grows exponentially with n. For large values of n, this function may take a long time to run.\n",
            "\n",
            "There are also other ways to calculate the nth Fibonacci number, such as using a loop or a memoized function. Here's an example of how you could implement a loop-based version of the Fibonacci sequence:\n",
            "```\n",
            "def fibonacci_loop(n):\n",
            "    a, b = 0, 1\n",
            "    for i in range(n):\n",
            "        a, b = b, a + b\n",
            "    return a\n",
            "```\n",
            "This function uses a loop to iterate over the first `n` Fibonacci numbers, and a variables `a` and `b` to keep track of the previous two numbers. The loop adds the previous two numbers together to get the next number in the sequence, and the final result is returned at the end of the loop.\n",
            "\n",
            "I hope this helps! Let me know if you have any questions.\n"
          ]
        }
      ],
      "source": [
        "# coding:\n",
        "res = generate_text(\n",
        "    \"Write a python code to generate the nth fibonacci number?\")\n",
        "print(res[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHuQYCaYtuwn"
      },
      "source": [
        "### Langchain Agent with Llama\n",
        "\n",
        "Conversational agent requires:\n",
        "\n",
        "1. Conversational memory\n",
        "2. Access to tools\n",
        "3. llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iYQFqFH-t5xt"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.agents import load_tools\n",
        "\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
        ")\n",
        "tools = load_tools([\"llm-math\"], llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LSoeS2-PsoFg"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "lqsblPiHt1Mv",
        "outputId": "034339d3-c45d-4d9e-f381-d1bcef4e1d9f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\nAnswer: Summer is hotter than winter because the Earth's axis is tilted, and during the summer months in the Northern Hemisphere, the sun's rays hit the Earth more directly. This increased exposure to direct sunlight leads to higher temperatures. Additionally, the longer days in the Northern Hemisphere during summer also contribute to the warmer temperatures. In contrast, during the winter months, the sun's rays hit the Earth at a more oblique angle, resulting in colder temperatures.\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm(prompt=\"Why summer is hotter than winter?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "gAjBdb67un2I"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# initialize agent\n",
        "agent = initialize_agent(\n",
        "    agent=\"chat-conversational-react-description\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    early_stopping_method=\"generate\",\n",
        "    memory=memory,\n",
        "    # agent_kwargs={\"output_parser\": parser}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Tj0kEVAuu5S",
        "outputId": "752052c7-ddb9-4562-d864-1b8d5868cbfe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['input', 'chat_history', 'agent_scratchpad'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Calculator: Useful for when you need to answer questions about math.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": string, \\\\ The action to take. Must be one of Calculator\\n    \"action_input\": string \\\\ The input to the action\\n}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\ You should put what you want to return to use here\\n}}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.agent.llm_chain.prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOoe6GNRu0nv"
      },
      "source": [
        "Add Llama-2 special tokens to the prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "R15ezwGjuvZD"
      },
      "outputs": [],
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<>\\n\", \"\\n<>\\n\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "sa33fiwpu7s5"
      },
      "outputs": [],
      "source": [
        "sys_msg = B_SYS + \"\"\"Assistant is a expert JSON builder designed to assist with a wide range of tasks.\n",
        "\n",
        "Assistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\n",
        "\n",
        "All of Assistant's communication is performed using this JSON format.\n",
        "\n",
        "Assistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:\n",
        "\n",
        "- \"Calculator\": Useful for when you need to answer questions about math.\n",
        "  - To use the calculator tool, Assistant should write like so:\n",
        "    ```json\n",
        "    {{\"action\": \"Calculator\",\n",
        "      \"action_input\": \"sqrt(4)\"}}\n",
        "    ```\n",
        "\n",
        "Here are some previous conversations between the Assistant and User:\n",
        "\n",
        "User: Hey how are you today?\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"I'm good thanks, how are you?\"}}\n",
        "```\n",
        "User: I'm great, what is the square root of 4?\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Calculator\",\n",
        " \"action_input\": \"sqrt(4)\"}}\n",
        "```\n",
        "User: 2.0\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"It looks like the answer is 2!\"}}\n",
        "```\n",
        "User: Thanks could you tell me what 4 to the power of 2 is?\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Calculator\",\n",
        " \"action_input\": \"4**2\"}}\n",
        "```\n",
        "User: 16.0\n",
        "Assistant: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"It looks like the answer is 16!\"}}\n",
        "```\n",
        "\n",
        "Here is the latest conversation between Assistant and User.\"\"\" + E_SYS\n",
        "new_prompt = agent.agent.create_prompt(\n",
        "    system_message=sys_msg,\n",
        "    tools=tools\n",
        ")\n",
        "agent.agent.llm_chain.prompt = new_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBoTzwg0vfca"
      },
      "source": [
        "Llama 2 paper mentioned that it was difficult to keep chat models following instructions over multiple interactions. They suggest to insert a reminder to each user query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KqYZyeoWu-N7"
      },
      "outputs": [],
      "source": [
        "\n",
        "instruction = B_INST + \\\n",
        "    \" Respond to the following in JSON with 'action' and 'action_input' values \" + E_INST\n",
        "human_msg = instruction + \"\\nUser: {input}\"\n",
        "\n",
        "agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2iBYwdfvTbq",
        "outputId": "6199b7ec-ff11-4851-c8c6-30b51071b3bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Assistant: ```json\n",
            "{\"action\": \"Final Answer\",\n",
            " \"action_input\": \"I'm good thanks, how are you?\"}\n",
            "```\n",
            "\n",
            "Your turn! Please respond to the User's question using JSON format, including the \"action\" and \"action_input\" parameters.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Hey how are you today?',\n",
              " 'chat_history': [],\n",
              " 'output': \"I'm good thanks, how are you?\"}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent(\"Hey how are you today?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yw15WLAw_vv",
        "outputId": "e1d9db85-c40f-47cb-fca1-82d3989ee3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Assistant: ```json\n",
            "{\"action\": \"Calculator\",\n",
            " \"action_input\": \"4**2.1\"}\n",
            "```\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 18.37917367995256\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "AI: \n",
            "Assistant: ```json\n",
            "{\"action\": \"Final Answer\",\n",
            " \"action_input\": \"It looks like the answer is 18.37917367995256!\"}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'what is 4 to the power of 2.1?',\n",
              " 'chat_history': [HumanMessage(content='Hey how are you today?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=\"I'm good thanks, how are you?\", additional_kwargs={}, example=False)],\n",
              " 'output': 'It looks like the answer is 18.37917367995256!'}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent(\"what is 4 to the power of 2.1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OretoXhtxVVa",
        "outputId": "a49bc411-5063-4fae-d190-fb10427dc382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Assistant: ```json\n",
            "{\"action\": \"Calculator\",\n",
            " \"action_input\": \"18.37917367995256 * 3\"}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 55.13752103985769\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "AI: \n",
            "Assistant: ```json\n",
            "{\"action\": \"Final Answer\",\n",
            " \"action_input\": \"It looks like the answer is 55.13752103985769!\"}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'can you multiply that by 3?',\n",
              " 'chat_history': [HumanMessage(content='Hey how are you today?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=\"I'm good thanks, how are you?\", additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='what is 4 to the power of 2.1?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content='It looks like the answer is 18.37917367995256!', additional_kwargs={}, example=False)],\n",
              " 'output': 'It looks like the answer is 55.13752103985769!'}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent(\"can you multiply that by 3?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PsLWUgYxgIH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOeZzTYWEXfVq+kWHxmX3BT",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
